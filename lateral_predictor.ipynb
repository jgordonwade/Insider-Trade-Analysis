{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ae832f",
   "metadata": {},
   "source": [
    "## Detection and Use of Insider Trading Information\n",
    "\n",
    "#### This is one of three notebooks comprising the capstone project.  \n",
    "\n",
    "\n",
    "#### The other two notebooks are: `get_insider_buys.ipynb`, and `insider_trade_detection.ipynb`.\n",
    "\n",
    "### The main notebook is  `insider_trade_detection.ipynb`\n",
    "\n",
    "\n",
    "* Purpose of this notebook: Perform the lateral prediction aspect of the project. \n",
    "* Input is a list of ticker symbols, via a file `tickerlist.csv`\n",
    "* Output is a `lrc_inferred.csv`\n",
    "\n",
    "This file  `insider_buys.csv` is then used in the notebook `insider_trade_detection`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6914185",
   "metadata": {},
   "source": [
    "#### Overall outline of lateral predictor.\n",
    "\n",
    "1. Read `ohlcv_all` and ensure the dates are all the same\n",
    "2. Get `lrc_all`. Make sure the set the initial values to 0 instead of NaN\n",
    "3. We'll made lateral predictions for each date in `prediction_dates`. So set up `prediction_dates`. Here's how.\n",
    "     - a. We want the `prediction_dates` to start as early as possible and end on the last day of available data. \n",
    "     - b. \"As early as possible:\" For each given target date `tdate`, we want to make the lateral predictions for all stocks for `tdate`, based on regression on the past `n_days_back` worth of data from the neighbors. This is the constraint on how far back we can set the beginning of `prediction_dates`.\n",
    "     - c. We also want to be able to see what happens after the `tdate`, `n_days_fwd` about 20, say. \n",
    "     - d. So what this means is that for any `tdate` in `prediction_dates`, I'm able to access data in `lrc_all` for `n_days_back` from tdate up to `n_days_fwd` from `tdate`.  \n",
    "     \n",
    "For each `tdate` in `prediction_dates`:\n",
    "\n",
    "4. Create `regression_dates`, the dates on which we'll do the regressions for tdate. \n",
    "    - a. The features are going to be lrc for each date up thru tdate\n",
    "    Create a np array consisting of the `lrc` column for all tickers and dates from (tdate - n_days_back) to (tdate - 1)\n",
    "    - b. Compute the distance matrix based on that np array\n",
    "    - c. For each target ticker:\n",
    "        - i. Use the distance matrix to get the neighborhood\n",
    "        - ii. Use the lrc data for each neighor, for all the regression_dates, to laterally predict the lrc value of the target ticker on the target date. \n",
    "        - iii. To check for the insider effect, compare the relative return over the next `n_days_fwd` for the neighborhood as a whole vs. that of the target stock. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "#import datetime\n",
    "import insider_trade_detector as itd\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pytz import timezone\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ce1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_datestamp(date_str):\n",
    "#     \"\"\"\n",
    "#     Convert a date string to a timezone-aware timestamp for the 'America/New_York' timezone.\n",
    "\n",
    "#     :param date_str: String representing the date in 'YYYY-MM-DD' format.\n",
    "#     :return: Timezone-aware datetime object for the 'America/New_York' timezone.\n",
    "#     \"\"\"\n",
    "#     # Parse the string into a datetime object\n",
    "#     naive_datetime = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "#     # Define the New York timezone\n",
    "#     new_york_tz = timezone(\"America/New_York\")\n",
    "\n",
    "#     # Localize the datetime object to New York timezone\n",
    "#     aware_datetime = new_york_tz.localize(naive_datetime)\n",
    "\n",
    "#     return aware_datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohlcv_all_to_lrc_all(ohlcv_all):\n",
    "    lrc_all = []\n",
    "    for ticker in ohlcv_all.index.get_level_values(0).unique():\n",
    "        ohlcv_single = ohlcv_all.xs(ticker, level='ticker')\n",
    "        previous_close = ohlcv_single['Close'].shift(1)\n",
    "        lrc = pd.DataFrame({'lrc': np.log(ohlcv_single['Close'] / previous_close)})\n",
    "        lrc.iloc[0] = 0\n",
    "        lrc_all.append(lrc.set_index([pd.Index([ticker] * len(lrc), name='ticker'), ohlcv_single.index]))\n",
    "    lrc_all_df = pd.concat(lrc_all)\n",
    "    return lrc_all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852cb7b",
   "metadata": {},
   "source": [
    "#### Data Acquisition and Structure\n",
    "In this section, we utilize `yfinance` to import OHLCV (Open, High, Low, Close, Volume) data for 40 selected stocks, and addionally QQQ and XLK, covering every trading day over the past 11 years. Ensure that we've got the same dates for all the stocks.\n",
    "\n",
    "The data is structured into a Pandas DataFrame named `ohlcv_all`. This DataFrame is indexed on two levels: `ticker` and `date`. and columns `Open`, `High`,  `Low`, `Close`,  and `Volume`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a13283",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_list = sorted(itd.read_tickerlist_csv('tickerlist_naz100.csv'))\n",
    "#symbol_list = symbol_list[20:26]\n",
    "symbol_list = list({'QQQ','XLK'}.union(set(symbol_list)))\n",
    "symbol_dict = { sym: yf.Ticker(sym) for sym in symbol_list }\n",
    "print(symbol_list)\n",
    "\n",
    "start_date, late_ticker = itd.get_start_date(symbol_list,start_date_pad=20)\n",
    "start_date = start_date.strftime('%Y-%m-%d')\n",
    "#start_date = '2020--01'; late_ticker=None\n",
    "\n",
    "print(len(symbol_list)-2, 'Nasdaq-100 tech stocks, plus QQQ and XLK. \\n')\n",
    "print(start_date, '\\t \"Latest\" ticker is', late_ticker)\n",
    "\n",
    "yesterday_date =  (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "\n",
    "ohlcv_all = pd.concat(\n",
    "    {sym : symbol_dict[sym].history(sym, start=start_date, end=yesterday_date, actions=False) for sym in symbol_dict},\n",
    "    names=['ticker', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that each ticker has the same date indices.\n",
    "unique_tickers = ohlcv_all.index.get_level_values('ticker').unique()\n",
    "first_ticker_dates = ohlcv_all.xs(unique_tickers[0], level='ticker').index\n",
    "all_dates_same = True\n",
    "for ticker in unique_tickers:\n",
    "    ticker_dates = ohlcv_all.xs(ticker, level='ticker').index\n",
    "    if not ticker_dates.equals(first_ticker_dates):\n",
    "        all_dates_same = False\n",
    "        print(f'\\t {ticker} has a wrong number of dates')\n",
    "if all_dates_same:\n",
    "    print(\"All tickers have the same Date indices.\")\n",
    "else:\n",
    "    print(\"Not all tickers have the same Date indices.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc89c7b",
   "metadata": {},
   "source": [
    "#### Computing Daily Log-Returns\n",
    "The next step involves calculating the daily log-returns for each stock on each trading day. Place the results in new DataFrame `lrc_all` with the same two-level indexing (ticker and date). \n",
    "\n",
    "This DataFrame serves as the foundation for the subsequent steps of identifying stock correlations and anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca050844",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_all = ohlcv_all_to_lrc_all(ohlcv_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f218644",
   "metadata": {},
   "source": [
    "Set some parameters and some working var's that depend on them. Key here is `prediction_dates`: We computed a predicted LRC value for each stock for each day in `prediction_dates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8147c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days_back = 200\n",
    "n_days_fwd = 10\n",
    "ewm_span = 200\n",
    "nbd_size = 3\n",
    "\n",
    "\n",
    "start_index_for_predictions = n_days_back + 1\n",
    "end_index_for_predictions = len(ticker_dates) - n_days_fwd\n",
    "prediction_dates = ticker_dates[start_index_for_predictions:end_index_for_predictions]\n",
    "\n",
    "ewm_alpha = 2 / (ewm_span + 1)\n",
    "ewm_weights = np.array([(1 - ewm_alpha) ** i for i in range(n_days_back)])\n",
    "ewm_weights = ewm_weights[::-1]\n",
    "ewm_weights = ewm_weights / ewm_weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88f58b",
   "metadata": {},
   "source": [
    "Initialze and run the main loop. Create a `LinearRegression` called `lateral_lateral_predictor` and use it as follows\n",
    "\n",
    "* For each `tdate` in `prediction_dates`:\n",
    "    * Compute the distance matrix \n",
    "    * For each `symb`:\n",
    "        - Find the `nbd_size` nearest neighbors\n",
    "        - Use the LinearRegression object to compute  the inferred LRC value for `symb`, `tdate` based the LRC's of the rest of the neighborhood. \n",
    "        - Record these inferred LRC values, and also the R^2-values for the training data and for the testing data.\n",
    "\n",
    "Main result of the loop go in `lrc_inferred`. \n",
    "\n",
    "This takes several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_inferred = pd.DataFrame(index=lrc_all.index, columns=['lrc_inferred', 'r2_train', 'r2_test'])\n",
    "lateral_predictor = LinearRegression()\n",
    "rsquareds = []\n",
    "testvals_5day = []\n",
    "for tdate in prediction_dates:\n",
    "\n",
    "    tdate_index = ticker_dates.get_loc(tdate)\n",
    "\n",
    "    traindates_start_index = tdate_index-n_days_back-1\n",
    "    traindates_end_index = tdate_index-1\n",
    "    train_dates = ticker_dates[traindates_start_index:traindates_end_index]\n",
    "    \n",
    "    testdates_start_index = tdate_index\n",
    "    testdates_end_index = tdate_index+n_days_fwd\n",
    "    test_dates = ticker_dates[testdates_start_index:testdates_end_index]\n",
    "\n",
    "\n",
    "    lrc_train = lrc_all.loc[(slice(None), train_dates),:]\n",
    "    lrc_test = lrc_all.loc[(slice(None), test_dates), :]\n",
    "    \n",
    "    distance_matrix =  1 - itd.corr_ewm(lrc_train,ewm_span)\n",
    "    \n",
    "    for symb in symbol_dict.keys():\n",
    "        nbrs,tightness =  itd.find_neighbors( distance_matrix, symb, k=nbd_size )\n",
    "        \n",
    "        X_train = lrc_train.loc[nbrs].unstack(level='ticker')['lrc'].values\n",
    "        y_train = lrc_train.loc[symb].values.squeeze()\n",
    "        lateral_predictor.fit(X_train, y_train, sample_weight=ewm_weights)\n",
    "        y_hat_train = lateral_predictor.predict(X_train)\n",
    "        residuals = y_train - y_hat_train\n",
    "        mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "        r2_train = lateral_predictor.score(X_train, y_train)\n",
    "        \n",
    "        X_test = lrc_test.loc[nbrs].unstack(level='ticker')['lrc'].values\n",
    "        y_test = lrc_test.loc[symb].values.squeeze()\n",
    "        y_predicted_test = lateral_predictor.predict(X_test)\n",
    "        residuals = y_test - y_predicted_test\n",
    "        mse_test = mean_squared_error(y_test, y_predicted_test)\n",
    "        if r2_train>0.5:\n",
    "            r2_test = lateral_predictor.score(X_test, y_test)\n",
    "        else:\n",
    "            r2_test = 0.0\n",
    "\n",
    "\n",
    "        rsquareds.append([r2_train, r2_test])\n",
    "        lrc_inferred.loc[(symb,tdate),'lrc_inferred'] = y_predicted_test[0]\n",
    "        lrc_inferred.loc[(symb,tdate),'r2_train'] = r2_train\n",
    "        lrc_inferred.loc[(symb,tdate),'r2_test'] = r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a663da7",
   "metadata": {},
   "source": [
    "Write `lrc_inferred` to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc941b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_inferred['lrc'] = lrc_all['lrc']\n",
    "lrc_inferred = lrc_inferred[['lrc', 'lrc_inferred', 'r2_train', 'r2_test']]\n",
    "lrc_inferred = lrc_inferred.loc[lrc_inferred.index.get_level_values('date').isin(prediction_dates)]\n",
    "lrc_inferred.shape\n",
    "lrc_inferred.reset_index().to_csv('lrc_inferred.csv', index=False)\n",
    "time.sleep(1)\n",
    "x = pd.read_csv('lrc_inferred.csv')\n",
    "x.set_index(['ticker', 'date'], inplace=True)\n",
    "x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aeba3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4aa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d1145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d449d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone Dev",
   "language": "python",
   "name": "capstone_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
